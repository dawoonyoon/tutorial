{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "242ecca8",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d75ba",
   "metadata": {},
   "source": [
    "# Introduction: Home Credit Default Risk Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4364c",
   "metadata": {},
   "source": [
    "The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task: \n",
    "* **Supervised**: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\n",
    "* **Classification**: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8631a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eae54e",
   "metadata": {},
   "source": [
    "The data is provided by [Home Credit](https://www.homecredit.net/about-us.aspx), a service dedicated to provided lines of credit (loans) to the unbanked population. Predicting whether or not a client will repay a loan or have difficulty is a critical business need, and Home Credit is hosting this competition on Kaggle to see what sort of models the machine learning community can develop to help them in this task.\n",
    "\n",
    "There are 7 different sources of data:\n",
    "\n",
    "* application_train/application_test: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature `SK_ID_CURR`. The training application data comes with the `TARGET` indicating 0: the loan was repaid or 1: the loan was not repaid.\n",
    "* bureau: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n",
    "* bureau_balance: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n",
    "* previous_application: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature `SK_ID_PREV`.\n",
    "* POS_CASH_BALANCE: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n",
    "* credit_card_balance: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n",
    "* installments_payment: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8637f8",
   "metadata": {},
   "source": [
    "![how all the data is related:](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b2e17",
   "metadata": {},
   "source": [
    "we are provided with the definitions of all the columns (in `HomeCredit_columns_description.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed6fca",
   "metadata": {},
   "source": [
    "## Metric: ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a138c04",
   "metadata": {},
   "source": [
    "Once we have a grasp of the data (reading through the [column descriptions](https://www.kaggle.com/c/home-credit-default-risk/data) helps immensely), we need to understand the metric by which our submission is judged. In this case, it is a common classification metric known as the [Receiver Operating Characteristic Area Under the Curve (ROC AUC, also sometimes called AUROC)](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it).\n",
    "\n",
    "The ROC AUC may sound intimidating, but it is relatively straightforward once you can get your head around the two individual concepts. The [Reciever Operating Characteristic (ROC) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) graphs the true positive rate versus the false positive rate:\n",
    "\n",
    "![image](http://www.statisticshowto.com/wp-content/uploads/2016/08/ROC-curve.png)\n",
    "\n",
    "A single line on the graph indicates the curve for a single model, and movement along a line indicates changing the threshold used for classifying a positive instance. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A curve that is to the left and above another curve indicates a better model. For example, the blue model is better than the red model, which is better than the black diagonal line which indicates a naive random guessing model. \n",
    "\n",
    "The [Area Under the Curve (AUC)](http://gim.unmc.edu/dxtests/roc3.htm) explains itself by its name! It is simply the area under the ROC curve. (This is the integral of the curve.) This metric is between 0 and 1 with a better model scoring higher. A model that simply guesses at random will have an ROC AUC of 0.5.\n",
    "\n",
    "When we measure a classifier according to the ROC AUC, we do not generation 0 or 1 predictions, but rather a probability between 0 and 1. This may be confusing because we usually like to think in terms of accuracy, but when we get into problems with inbalanced classes (we will see this is the case), accuracy is not the best metric. For example, if I wanted to build a model that could detect terrorists with 99.9999% accuracy, I would simply make a model that predicted every single person was not a terrorist. Clearly, this would not be effective (the recall would be zero) and we use more advanced metrics such as ROC AUC or the [F1 score](https://en.wikipedia.org/wiki/F1_score) to more accurately reflect the performance of a classifier. A model with a high ROC AUC will also have a high accuracy, but the [ROC AUC is a better representation of model performance.](https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy)\n",
    "\n",
    "Not that we know the background of the data we are using and the metric to maximize, let's get into exploring the data. In this notebook, as mentioned previously, we will stick to the main data sources and simple models which we can build upon in future work. \n",
    "\n",
    "__Follow-up Notebooks__\n",
    "\n",
    "For those looking to keep working on this problem, I have a series of follow-up notebooks:\n",
    "\n",
    "* [Manual Feature Engineering Part One](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)\n",
    "* [Manual Feature Engineering Part Two](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2)\n",
    "* [Introduction to Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics)\n",
    "* [Advanced Automated Feature Engineering](https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory)\n",
    "* [Feature Selection](https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection)\n",
    "* [Intro to Model Tuning: Grid and Random Search](https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search)\n",
    "* [Automated Model Tuning](https://www.kaggle.com/willkoehrsen/automated-model-tuning)\n",
    "* [Model Tuning Results](https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6ee51",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7df6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fc3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
